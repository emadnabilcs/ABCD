{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camel_tools\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.utils.normalize import (\n",
    "    normalize_alef_maksura_ar,\n",
    "    normalize_teh_marbuta_ar,\n",
    "    normalize_ligatures_ar,\n",
    "    normalize_hamza_ar,\n",
    "    normalize_unicode_ar,\n",
    "    normalize_hamza_norm_ar,\n",
    "    normalize_hamza_above_ar,\n",
    "    normalize_hamza_below_ar,\n",
    "    normalize_tatweel_ar,\n",
    "    normalize_yah_ar,\n",
    "    normalize_waw_ar,\n",
    "    normalize_shadda_ar,\n",
    "    normalize_fatha_ar,\n",
    "    normalize_damma_ar,\n",
    "    normalize_kasra_ar,\n",
    "    normalize_sukun_ar,\n",
    "    normalize_tanwin_ar,\n",
    "    normalize_tashkeel_ar,\n",
    "    normalize_lamaleph_ar,\n",
    "    normalize_lamaleph_norm_ar,\n",
    "    normalize_lamaleph_above_ar,\n",
    "    normalize_lamaleph_below_ar,\n",
    "    normalize_lamaleph_shadda_ar,\n",
    "    normalize_lamaleph_shadda_norm_ar,\n",
    "    normalize_lamaleph_shadda_above_ar,\n",
    "    normalize_lamaleph_shadda_below_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_norm_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_above_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_below_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_norm_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_above_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_below_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_norm_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_above_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_below_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_norm_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_above_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_below_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_norm_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_above_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_below_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_shadda_ar,\n",
    "    normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_shadda_norm_ar\n",
    "    # ... (import other normalization functions)\n",
    ")\n",
    "\n",
    "def clean_and_normalize_arabic_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize Arabic text using `CaMEL` library.\n",
    "    \"\"\"\n",
    "    words = simple_word_tokenize(text)\n",
    "    \n",
    "    # Apply various cleaning and normalization functions\n",
    "    cleaned_words = [dediac_ar(word) for word in words]\n",
    "    cleaned_words = [normalize_alef_maksura_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_teh_marbuta_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_ligatures_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_hamza_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_unicode_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_hamza_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_hamza_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_hamza_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_tatweel_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_yah_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_waw_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_shadda_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_fatha_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_damma_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_kasra_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_sukun_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_tanwin_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_tashkeel_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_norm_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_above_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_below_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_shadda_ar(word) for word in cleaned_words]\n",
    "    cleaned_words = [normalize_lamaleph_shadda_tashkeel_shadda_tashkeel_shadda_tashkeel_shadda_norm_ar(word) for word in cleaned_words]\n",
    "\n",
    "    # Join the cleaned words back into a text string\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Read dataset\n",
    "data = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with the path to your dataset\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Clean and normalize the Arabic text in the 'Matn' column\n",
    "train_data['Cleaned_Text'] = train_data['Matn'].apply(clean_and_normalize_arabic_text)\n",
    "test_data['Cleaned_Text'] = test_data['Matn'].apply(clean_and_normalize_arabic_text)\n",
    "\n",
    "# TF-IDF Vectorization with n-gram support (unigrams, bigrams, trigrams)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data['Cleaned_Text'])\n",
    "X_test = tfidf_vectorizer.transform(test_data['Cleaned_Text'])\n",
    "\n",
    "# Initialize models with specified hyperparameters\n",
    "models = {\n",
    "    'GaussianNB': GaussianNB(var_smoothing=1e-9),\n",
    "    'LogisticRegression': LogisticRegression(penalty='l2', multi_class='auto'),\n",
    "    'SVM': SVC(kernel='rbf', C=4, gamma=0.125),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(100), batch_size='auto', activation='relu', solver='adam')\n",
    "}\n",
    "\n",
    "# Train and test models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, train_data['Label'])\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics (precision, recall, f1-score)\n",
    "    metrics = classification_report(test_data['Label'], predictions)\n",
    "    print(f\"Metrics for {name}:\")\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de990daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code to find the best hyperparameters for the models.\n",
    "# You can use GridSearchCV from the sklearn.model_selection module. \n",
    "# This allows you to perform an exhaustive search over a specified parameter grid to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Rest of your code to load the data and clean text remains the same\n",
    "\n",
    "# Parameters grid for Logistic Regression\n",
    "param_grid_logreg = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# GridSearchCV for Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, multi_class='auto')\n",
    "grid_search_logreg = GridSearchCV(logreg, param_grid=param_grid_logreg, cv=3, scoring='accuracy')\n",
    "grid_search_logreg.fit(X_train, train_data['Label'])\n",
    "\n",
    "# Best parameters and best score for Logistic Regression\n",
    "print(\"Best Parameters for Logistic Regression:\", grid_search_logreg.best_params_)\n",
    "print(\"Best Score for Logistic Regression:\", grid_search_logreg.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fad7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Parameters grid for GaussianNB (Gaussian Naive Bayes doesn't have many hyperparameters)\n",
    "param_grid_gnb = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]  # You can adjust these values based on your needs\n",
    "}\n",
    "\n",
    "# GridSearchCV for GaussianNB\n",
    "gnb = GaussianNB()\n",
    "grid_search_gnb = GridSearchCV(gnb, param_grid=param_grid_gnb, cv=3, scoring='accuracy')\n",
    "grid_search_gnb.fit(X_train.toarray(), train_data['Label'])\n",
    "\n",
    "# Best parameters and best score for GaussianNB\n",
    "print(\"Best Parameters for GaussianNB:\", grid_search_gnb.best_params_)\n",
    "print(\"Best Score for GaussianNB:\", grid_search_gnb.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8137d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Parameters grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "    # Add more parameters as needed\n",
    "}\n",
    "\n",
    "# GridSearchCV for SVM\n",
    "svm = SVC()\n",
    "grid_search_svm = GridSearchCV(svm, param_grid=param_grid_svm, cv=3, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train, train_data['Label'])\n",
    "\n",
    "# Best parameters and best score for SVM\n",
    "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(\"Best Score for SVM:\", grid_search_svm.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33629a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Parameters grid for MLP\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(100,), (50,50), (100,50,100)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'activation': ['logistic', 'tanh', 'relu']\n",
    "    # Add more parameters as needed\n",
    "}\n",
    "\n",
    "# GridSearchCV for MLP\n",
    "mlp = MLPClassifier(max_iter=500)\n",
    "grid_search_mlp = GridSearchCV(mlp, param_grid=param_grid_mlp, cv=3, scoring='accuracy')\n",
    "grid_search_mlp.fit(X_train, train_data['Label'])\n",
    "\n",
    "# Best parameters and best score for MLP\n",
    "print(\"Best Parameters for MLP:\", grid_search_mlp.best_params_)\n",
    "print(\"Best Score for MLP:\", grid_search_mlp.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
